{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPG Examples\n",
    "This document shows how to use the PyTPG API. We make use of OpenAI Gym to run examples, and we assume you already have PyTPG installed, see the readme for installation instructions for PyTPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython.display import clear_output\n",
    "# imports to run OpenAI Gym in Jupyter\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# how to render in Jupyter: \n",
    "# https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server\n",
    "# https://www.youtube.com/watch?v=O84KgRt6AJI\n",
    "def show_state(env, step=0, name='', info=''):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (name, step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "# transforms the state into what the tpg agent can use.\n",
    "# From 3D to 1D, taking only red data (from rgb array)\n",
    "def getState(state):\n",
    "    state2 = []\n",
    "    for x in state:\n",
    "        for y in x:\n",
    "            state2.append(y[0])\n",
    "            \n",
    "    return state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/git/gym/gym/__init__.py:22: UserWarning: DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.\n",
      "  warnings.warn('DEPRECATION WARNING: to improve load times, gym no longer automatically loads gym.spaces. Please run \"import gym.spaces\" to load gym.spaces on your own. This warning will turn into an error in a future version of gym.')\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Assault-v0') # make the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # learn size of action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to do training\n",
    "from tpg.tpg_trainer import TpgTrainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.tpg_agent import TpgAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Basic Generational Selection (with graphics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89b702a73220>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutcome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get initial state and prep environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# run episodes that last 200 frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time # for tracking time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "# first create an instance of the TpgTrainer\n",
    "# this creates the whole population and everything\n",
    "trainer = TpgTrainer(actions=range(7), teamPopSizeInit=50)\n",
    "\n",
    "curScores = [] # hold scores in a generation\n",
    "summaryScores = [] # record score summaries for each gen (min, max, avg)\n",
    "\n",
    "# 5 generations isn't much (not even close), but some improvements\n",
    "# should be seen.\n",
    "for gen in range(5): # generation loop\n",
    "    curScores = [] # new list per gen\n",
    "    \n",
    "    while True: # loop to go through agents\n",
    "        teamNum = trainer.remainingAgents()\n",
    "        agent = trainer.getNextAgent()\n",
    "        if agent is None:\n",
    "            break # no more agents, so proceed to next gen\n",
    "        \n",
    "        # check if agent already has score\n",
    "        if agent.taskDone():\n",
    "            score = agent.getOutcome()\n",
    "        else:\n",
    "            state = env.reset() # get initial state and prep environment\n",
    "            score = 0\n",
    "            for i in range(200): # run episodes that last 200 frames\n",
    "                show_state(env, i, 'Assault', 'Gen #' + str(gen) + \n",
    "                           ', Team #' + str(teamNum) +\n",
    "                           ', Score: ' + str(score)) # render env\n",
    "\n",
    "                act = agent.act(getState(state)) # get action from agent\n",
    "\n",
    "                # feedback from env\n",
    "                state, reward, isDone, debug = env.step(act)\n",
    "                score += reward # accumulate reward in score\n",
    "                if isDone:\n",
    "                    break # end early if losing state\n",
    "\n",
    "            agent.reward(score) # must reward agent (if didn't already score)\n",
    "            \n",
    "        curScores.append(score) # store score\n",
    "            \n",
    "    # at end of generation, make summary of scores\n",
    "    summaryScores.append((min(curScores), max(curScores),\n",
    "                    sum(curScores)/len(curScores))) # min, max, avg\n",
    "    trainer.evolve()\n",
    "    \n",
    "#clear_output(wait=True)\n",
    "print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for result in summaryScores:\n",
    "    print(result[0],result[1],result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Generational Selection with Multiprocessing (no graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is just to show a different way to run the API. It uses a different method to get the agents, doesn't use graphics, and uses multiprocessing. For a more in depth comparison of run performances with different configurations see {Section Name}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run agent in function to work with multiprocessing\n",
    "def runAgent(agenteqsq):\n",
    "    agent = agenteqsq[0] # get agent\n",
    "    eq = agenteqsq[1] # get environment queue\n",
    "    sq = agenteqsq[2] # get score queue\n",
    "    \n",
    "    # check if agent already has score\n",
    "    if agent.taskDone():\n",
    "        print('Agent #' + str(agent.getAgentNum()) + ' can skip.')\n",
    "        return\n",
    "        \n",
    "    print('envs in queue:',eq.qsize())\n",
    "    env = eq.get() # get an environment\n",
    "    state = env.reset() # get initial state and prep environment\n",
    "    score = 0\n",
    "    for i in range(1000): # run episodes that last 200 frames\n",
    "        act = agent.act(getState(state)) # get action from agent\n",
    "\n",
    "        # feedback from env\n",
    "        state, reward, isDone, debug = env.step(act)\n",
    "        score += reward # accumulate reward in score\n",
    "        if isDone:\n",
    "            break # end early if losing state\n",
    "            \n",
    "    lock.acquire() # may not actually need, mp is weird in python\n",
    "    agent.reward(score) # must reward agent\n",
    "    lock.release()\n",
    "    \n",
    "    print('Agent #' + str(agent.getAgentNum()) + ' finished with score ' + str(score))\n",
    "    sq.put((agent.getUid(), agent.getOutcomes())) # get outcomes with id\n",
    "    eq.put(env) # put environment back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (Seconds): 4178.15893507\n",
      "Results so far: [(0.0, 546.0, 50.69166666666667), (0.0, 630.0, 88.01273885350318), (0.0, 630.0, 159.57042253521126), (0.0, 630.0, 223.85106382978722), (0.0, 630.0, 295.5272727272727), (0.0, 630.0, 319.07462686567163), (0.0, 630.0, 357.85873605947955), (0.0, 630.0, 365.4792452830189), (0.0, 630.0, 381.39473684210526), (0.0, 630.0, 385.5326086956522)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-4:\n",
      "Process PoolWorker-5:\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Process PoolWorker-2:\n",
      "    self.run()\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    self.run()\n",
      "    task = get()\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "KeyboardInterrupt\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    task = get()\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    racquire()\n",
      "    return recv()\n",
      "    task = get()\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    self.run()\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/random.py\", line 97, in __init__\n",
      "KeyboardInterrupt\n",
      "    self.seed(x)\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/random.py\", line 120, in seed\n",
      "    super(Random, self).seed(a)\n",
      "KeyboardInterrupt\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/pool.py\", line 102, in worker\n",
      "    task = get()\n",
      "  File \"/home/ryan/anaconda3/envs/oaigym/lib/python2.7/multiprocessing/queues.py\", line 374, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "lock = mp.Lock()\n",
    "\n",
    "trainer = TpgTrainer(actions=range(7), teamPopSizeInit=360)\n",
    "\n",
    "processes = 4 # how many to run concurrently (4 is best for my local desktop)\n",
    "\n",
    "m = mp.Manager()\n",
    "envQueue = m.Queue()\n",
    "# each process needs its own environment\n",
    "for i in range(processes):\n",
    "    envQueue.put(gym.make('Assault-v0'))\n",
    "\n",
    "pool = mp.Pool(processes=processes)\n",
    "    \n",
    "summaryScores = [] # record score summaries for each gen (min, max, avg)\n",
    "    \n",
    "    \n",
    "for gen in range(20): # generation loop\n",
    "    scoreQueue = m.Queue() # hold agents when finish, to actually apply score\n",
    "\n",
    "    # run generation\n",
    "    # skipTasks=[] so we get all agents, even if already scored,\n",
    "    # just to report the obtained score for all agents.\n",
    "    pool.map(runAgent, \n",
    "                 [(agent, envQueue, scoreQueue) \n",
    "                  for agent in trainer.getAllAgents(skipTasks=[])])\n",
    "    \n",
    "    scores = [] # convert scores into list\n",
    "    while not scoreQueue.empty():\n",
    "        scores.append(scoreQueue.get())\n",
    "    \n",
    "    # apply scores\n",
    "    trainer.applyScores(scores)\n",
    "    trainer.evolve(tasks=[]) # go into next gen\n",
    "    \n",
    "    scoreStats = trainer.scoreStats\n",
    "\n",
    "    # at end of generation, make summary of scores\n",
    "    summaryScores.append((trainer.scoreStats['min'], \n",
    "                    trainer.scoreStats['max'],\n",
    "                    trainer.scoreStats['average'])) # min, max, avg\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "    print('Results so far: ' + str(summaryScores))\n",
    "    \n",
    "clear_output(wait=True)\n",
    "print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "print('Results: ' + str(summaryScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tournament Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "lock = mp.Lock()\n",
    "\n",
    "trainer = TpgTrainer(actions=range(7), teamPopSizeInit=360)\n",
    "\n",
    "processes = 4 # how many to run concurrently (4 is best for my local desktop)\n",
    "\n",
    "m = mp.Manager()\n",
    "envQueue = m.Queue()\n",
    "# each process needs its own environment\n",
    "for i in range(processes):\n",
    "    envQueue.put(gym.make('Assault-v0'))\n",
    "\n",
    "pool = mp.Pool(processes=processes)\n",
    "    \n",
    "summaryScores = [] # record score summaries for population\n",
    "    \n",
    "# tournament loop \n",
    "# 450 tournaments of size 8 approximately equals 10 generations\n",
    "# *at pop size 360\n",
    "for tourn in range(450): \n",
    "    scoreQueue = m.Queue() # hold agents when finish, to actually apply score\n",
    "\n",
    "    # run tournament\n",
    "    # skipTasks=[] so we get all agents, even if already scored,\n",
    "    # just to report the obtained score for all agents.\n",
    "    pool.map(runAgent, \n",
    "                 [(agent, envQueue, scoreQueue) \n",
    "                  for agent in trainer.getTournamentAgents(skipTasks=[])])\n",
    "    \n",
    "    scores = [] # convert scores into list\n",
    "    while not scoreQueue.empty():\n",
    "        scores.append(scoreQueue.get())\n",
    "    \n",
    "    # apply scores\n",
    "    trainer.applyScores(scores)\n",
    "    trainer.evolve(tasks=[]) # evolve tournament players\n",
    "    \n",
    "    scoreStats = trainer.scoreStats\n",
    "\n",
    "    # at end of generation, make summary of scores\n",
    "    summaryScores.append((trainer.scoreStats['min'], \n",
    "                    trainer.scoreStats['max'],\n",
    "                    trainer.scoreStats['average'])) # min, max, avg\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "    print('Results so far: ' + str(summaryScores))\n",
    "    \n",
    "clear_output(wait=True)\n",
    "print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "print('Results: ' + str(summaryScores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oaigym]",
   "language": "python",
   "name": "conda-env-oaigym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPG Examples\n",
    "This document shows how to use the PyTPG API. We make use of OpenAI Gym to run examples, and we assume you already have PyTPG installed, see the readme for installation instructions for PyTPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# how to render in Jupyter: \n",
    "# https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server\n",
    "# https://www.youtube.com/watch?v=O84KgRt6AJI\n",
    "def show_state(env, step=0, name='', info=''):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (name, step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "# To transform pixel matrix to a single vector.\n",
    "def getState(inState):\n",
    "    # each row is all 1 color\n",
    "    rgbRows = np.reshape(inState,(len(inState[0])*len(inState), 3)).T\n",
    "\n",
    "    # add each with appropriate shifting\n",
    "    # get RRRRRRRR GGGGGGGG BBBBBBBB\n",
    "    return np.add(np.left_shift(rgbRows[0], 16),\n",
    "        np.add(np.left_shift(rgbRows[1], 8), rgbRows[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Assault-v0') # make the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # learn size of action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import to do training\n",
    "from tpg.trainer import Trainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Basic Generational Selection (with graphics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# takes too long with rendering so I just ignore this one, but it works\n",
    "\n",
    "import time # for tracking time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "# first create an instance of the TpgTrainer\n",
    "# this creates the whole population and everything\n",
    "# teamPopSize should realistically be at-least 100\n",
    "trainer = Trainer(actions=range(7), teamPopSize=20, rTeamPopSize=20) \n",
    "\n",
    "curScores = [] # hold scores in a generation\n",
    "summaryScores = [] # record score summaries for each gen (min, max, avg)\n",
    "\n",
    "# 5 generations isn't much (not even close), but some improvements\n",
    "# should be seen.\n",
    "for gen in range(5): # generation loop\n",
    "    curScores = [] # new list per gen\n",
    "    \n",
    "    agents = trainer.getAgents()\n",
    "    \n",
    "    while True: # loop to go through agents\n",
    "        teamNum = len(agents)\n",
    "        agent = agents.pop()\n",
    "        if agent is None:\n",
    "            break # no more agents, so proceed to next gen\n",
    "        \n",
    "        state = env.reset() # get initial state and prep environment\n",
    "        score = 0\n",
    "        for i in range(500): # run episodes that last 500 frames\n",
    "            show_state(env, i, 'Assault', 'Gen #' + str(gen) + \n",
    "                       ', Team #' + str(teamNum) +\n",
    "                       ', Score: ' + str(score)) # render env\n",
    "            \n",
    "            # get action from agent\n",
    "            # must transform to at-least int-32 (for my getState to bitshift correctly)\n",
    "            act = agent.act(getState(np.array(state, dtype=np.int32))) \n",
    "\n",
    "            # feedback from env\n",
    "            state, reward, isDone, debug = env.step(act)\n",
    "            score += reward # accumulate reward in score\n",
    "            if isDone:\n",
    "                break # end early if losing state\n",
    "\n",
    "        agent.reward(score) # must reward agent (if didn't already score)\n",
    "            \n",
    "        curScores.append(score) # store score\n",
    "        \n",
    "        if len(agents) == 0:\n",
    "            break\n",
    "            \n",
    "    # at end of generation, make summary of scores\n",
    "    summaryScores.append((min(curScores), max(curScores),\n",
    "                    sum(curScores)/len(curScores))) # min, max, avg\n",
    "    trainer.evolve()\n",
    "    \n",
    "#clear_output(wait=True)\n",
    "print('Time Taken (Hours): ' + str((time.time() - tStart)/3600))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for result in summaryScores:\n",
    "    print(result[0],result[1],result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Generational Selection with Multiprocessing (no graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is just to show a different way to run the API, a far superior way. It uses a different method to get the agents, doesn't use graphics (but can), and uses multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run each agent in this method for parallization.\n",
    "Args:\n",
    "    args: (TpgAgent, envName, scoreList, numEpisodes, numFrames)\n",
    "\"\"\"\n",
    "def runAgent(args):\n",
    "    agent = args[0]\n",
    "    envName = args[1]\n",
    "    scoreList = args[2]\n",
    "    numEpisodes = args[3] # number of times to repeat game\n",
    "    numFrames = args[4] \n",
    "    \n",
    "    # skip if task already done by agent\n",
    "    if agent.taskDone(envName):\n",
    "        print('Agent #' + str(agent.agentNum) + ' can skip.')\n",
    "        scoreList.append((agent.team.id, agent.team.outcomes))\n",
    "        return\n",
    "    \n",
    "    env = gym.make(envName)\n",
    "    valActs = range(env.action_space.n) # valid actions, some envs are less\n",
    "    \n",
    "    scoreTotal = 0 # score accumulates over all episodes\n",
    "    for ep in range(numEpisodes): # episode loop\n",
    "        state = env.reset()\n",
    "        scoreEp = 0\n",
    "        numRandFrames = 0\n",
    "        if numEpisodes > 1:\n",
    "            numRandFrames = random.randint(0,30)\n",
    "        for i in range(numFrames): # frame loop\n",
    "            if i < numRandFrames:\n",
    "                _, _, isDone, _ = env.step(env.action_space.sample())\n",
    "                if isDone: # don't count it if lose on random steps\n",
    "                    ep -= 1\n",
    "                continue\n",
    "\n",
    "            act = agent.act(getState(state))\n",
    "\n",
    "            # feedback from env\n",
    "            state, reward, isDone, debug = env.step(act)\n",
    "            scoreEp += reward # accumulate reward in score\n",
    "            if isDone:\n",
    "                break # end early if losing state\n",
    "                \n",
    "        print('Agent #' + str(agent.agentNum) + \n",
    "              ' | Ep #' + str(ep) + ' | Score: ' + str(scoreEp))\n",
    "        scoreTotal += scoreEp\n",
    "       \n",
    "    scoreTotal /= numEpisodes\n",
    "    env.close()\n",
    "    agent.reward(scoreTotal, envName)\n",
    "    scoreList.append((agent.team.id, agent.team.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (Hours): 2.909954893456565\n",
      "Results:\n",
      "Min, Max, Avg\n",
      "-100.0 -2.0 -46.915\n",
      "-100.0 2.0 -34.775\n",
      "-100.0 2.0 -36.17412935323383\n",
      "-100.0 2.0 -29.800995024875622\n",
      "-100.0 2.0 -27.437810945273633\n",
      "-100.0 4.0 -25.686567164179106\n",
      "-100.0 4.0 -24.203980099502488\n",
      "-100.0 4.0 -24.228855721393035\n",
      "-100.0 4.0 -19.104477611940297\n",
      "-100.0 4.0 -19.98\n",
      "-100.0 5.0 -17.39\n",
      "-100.0 8.0 -16.895\n",
      "-100.0 8.0 -16.885\n",
      "-100.0 10.0 -14.681592039800995\n",
      "-100.0 12.0 -11.88\n",
      "-100.0 13.0 -12.55\n",
      "-100.0 13.0 -16.38\n",
      "-100.0 19.0 -13.39\n",
      "-100.0 19.0 -13.119402985074627\n",
      "-100.0 19.0 -10.125\n",
      "-100.0 19.0 -12.09\n",
      "-100.0 19.0 -9.325\n",
      "-100.0 19.0 -9.185\n",
      "-100.0 19.0 -10.635\n",
      "-100.0 22.0 -7.52\n",
      "-100.0 22.0 -10.485\n",
      "-100.0 22.0 -10.425\n",
      "-100.0 22.0 -10.368159203980099\n",
      "-100.0 22.0 -9.9\n",
      "-100.0 22.0 -8.58\n",
      "-100.0 22.0 -10.45273631840796\n",
      "-100.0 22.0 -9.73\n",
      "-100.0 22.0 -8.405\n",
      "-100.0 30.0 -11.64\n",
      "-100.0 30.0 -9.2\n",
      "-100.0 30.0 -11.49\n",
      "-100.0 30.0 -8.475\n",
      "-100.0 30.0 -10.59\n",
      "-100.0 30.0 -8.83\n",
      "-100.0 30.0 -9.805\n",
      "-100.0 30.0 -7.517412935323383\n",
      "-100.0 30.0 -8.7\n",
      "-100.0 30.0 -7.069651741293533\n",
      "-100.0 30.0 -11.96\n",
      "-100.0 30.0 -4.335\n",
      "-100.0 30.0 -4.04\n",
      "-100.0 30.0 -7.105\n",
      "-100.0 30.0 -6.725\n",
      "-100.0 30.0 -6.42\n",
      "-100.0 30.0 -10.205\n",
      "-100.0 30.0 -9.104477611940299\n",
      "-100.0 30.0 -3.855\n",
      "-100.0 30.0 -10.505\n",
      "-100.0 30.0 -5.1940298507462686\n",
      "-100.0 30.0 -7.055\n",
      "-100.0 30.0 -4.94\n",
      "-100.0 30.0 -5.26\n",
      "-100.0 30.0 -6.268656716417911\n",
      "-100.0 30.0 -10.626865671641792\n",
      "-100.0 30.0 -5.134328358208955\n",
      "-100.0 30.0 -5.587064676616915\n",
      "-100.0 30.0 -6.2835820895522385\n",
      "-100.0 35.0 -4.36318407960199\n",
      "-100.0 35.0 -5.91\n",
      "-100.0 35.0 -4.766169154228856\n",
      "-100.0 35.0 -2.49\n",
      "-100.0 35.0 -5.21\n",
      "-100.0 35.0 -6.545\n",
      "-100.0 35.0 -4.065\n",
      "-100.0 35.0 -2.665\n",
      "-100.0 35.0 -6.255\n",
      "-100.0 35.0 -6.940298507462686\n",
      "-100.0 35.0 -7.375\n",
      "-100.0 35.0 -1.955223880597015\n",
      "-100.0 35.0 -2.02\n",
      "-74.0 35.0 -2.475\n",
      "-100.0 35.0 -2.135\n",
      "-100.0 35.0 -6.01\n",
      "-100.0 35.0 -6.37\n",
      "-100.0 35.0 -5.38\n",
      "-100.0 35.0 -2.0\n",
      "-100.0 35.0 -3.115\n",
      "-100.0 35.0 -5.135\n",
      "-100.0 35.0 -4.885\n",
      "-100.0 35.0 -2.375\n",
      "-100.0 35.0 -2.44\n",
      "-100.0 35.0 -5.1\n",
      "-100.0 35.0 -1.84\n",
      "-100.0 35.0 -5.51\n",
      "-100.0 35.0 -2.08\n",
      "-100.0 35.0 -4.45273631840796\n",
      "-100.0 35.0 -4.785\n",
      "-100.0 35.0 -2.635\n",
      "-100.0 35.0 -3.6\n",
      "-100.0 35.0 -2.255\n",
      "-100.0 35.0 -6.24\n",
      "-100.0 35.0 -1.6\n",
      "-100.0 35.0 -1.97\n",
      "-100.0 35.0 -2.1343283582089554\n",
      "-100.0 35.0 -1.8308457711442787\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "envName = 'Boxing-v0'\n",
    "# get num actions\n",
    "env = gym.make(envName)\n",
    "acts = env.action_space.n\n",
    "del env\n",
    "\n",
    "trainer = Trainer(actions=range(acts), teamPopSize=200, rTeamPopSize=200)\n",
    "\n",
    "processes = 7\n",
    "man = mp.Manager()\n",
    "pool = mp.Pool(processes=processes, maxtasksperchild=1)\n",
    "    \n",
    "allScores = [] # track all scores each generation\n",
    "\n",
    "for gen in range(100): # do 100 generations of training\n",
    "    scoreList = man.list()\n",
    "    \n",
    "    # get agents, noRef to not hold reference to trainer in each one\n",
    "    # don't need reference to trainer in multiprocessing\n",
    "    agents = trainer.getAgents() # swap out agents only at start of generation\n",
    "\n",
    "    # run the agents\n",
    "    pool.map(runAgent, \n",
    "        [(agent, envName, scoreList, 1, 18000)\n",
    "        for agent in agents])\n",
    "    \n",
    "    # apply scores, must do this when multiprocessing\n",
    "    # because agents can't refer to trainer\n",
    "    teams = trainer.applyScores(scoreList)\n",
    "    # important to remember to set tasks right, unless not using task names\n",
    "    # task name set in runAgent()\n",
    "    trainer.evolve(task=envName) # go into next gen\n",
    "    \n",
    "    # an easier way to track stats than the above example\n",
    "    scoreStats = trainer.fitnessStats\n",
    "    allScores.append((scoreStats['min'], scoreStats['max'], scoreStats['average']))\n",
    "    \n",
    "    clear_output()\n",
    "    print('Time Taken (Hours): ' + str((time.time() - tStart)/3600))\n",
    "    print('Results so far: ' + str(allScores))\n",
    "    \n",
    "clear_output()\n",
    "print('Time Taken (Hours): ' + str((time.time() - tStart)/3600))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for score in allScores:\n",
    "    print(score[0],score[1],score[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oaigym]",
   "language": "python",
   "name": "conda-env-oaigym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

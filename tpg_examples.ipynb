{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPG Examples\n",
    "This document shows how to use the PyTPG API. We make use of OpenAI Gym to run examples, and we assume you already have PyTPG installed, see the readme for installation instructions for PyTPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# how to render in Jupyter: \n",
    "# https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server\n",
    "# https://www.youtube.com/watch?v=O84KgRt6AJI\n",
    "def show_state(env, step=0, name='', info=''):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (name, step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "# To transform pixel matrix to a single vector.\n",
    "def getState(inState):\n",
    "    # each row is all 1 color\n",
    "    rgbRows = np.reshape(inState,(len(inState[0])*len(inState), 3)).T\n",
    "\n",
    "    # add each with appropriate shifting\n",
    "    # get RRRRRRRR GGGGGGGG BBBBBBBB\n",
    "    return np.add(np.left_shift(rgbRows[0], 16),\n",
    "        np.add(np.left_shift(rgbRows[1], 8), rgbRows[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Assault-v0') # make the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # learn size of action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to do training\n",
    "from tpg.tpg_trainer import TpgTrainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.tpg_agent import TpgAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Basic Generational Selection (with graphics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0fc2336f7a6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                 show_state(env, i, 'Assault', 'Gen #' + str(gen) + \n\u001b[1;32m     32\u001b[0m                            \u001b[0;34m', Team #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteamNum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                            ', Score: ' + str(score)) # render env\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# must transform to at-least int-32 (for my getState to bitshift correctly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-b37d46b3bc75>\u001b[0m in \u001b[0;36mshow_state\u001b[0;34m(env, step, name, info)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# To transform pixel matrix to a single vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2261\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2263\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m   2264\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/oaigym/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[0;32m--> 528\u001b[0;31m                                self.figure.dpi, metadata=metadata)\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_dpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time # for tracking time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "# first create an instance of the TpgTrainer\n",
    "# this creates the whole population and everything\n",
    "trainer = TpgTrainer(actions=range(7), teamPopSize=20, rTeamPopSize=20) # teamPopSize should realistically be at-least 100\n",
    "\n",
    "curScores = [] # hold scores in a generation\n",
    "summaryScores = [] # record score summaries for each gen (min, max, avg)\n",
    "\n",
    "# 5 generations isn't much (not even close), but some improvements\n",
    "# should be seen.\n",
    "for gen in range(5): # generation loop\n",
    "    curScores = [] # new list per gen\n",
    "    \n",
    "    while True: # loop to go through agents\n",
    "        teamNum = trainer.remainingAgents()\n",
    "        agent = trainer.getNextAgent()\n",
    "        if agent is None:\n",
    "            break # no more agents, so proceed to next gen\n",
    "        \n",
    "        # check if agent already has score\n",
    "        if agent.taskDone():\n",
    "            score = agent.getOutcome()\n",
    "        else:\n",
    "            state = env.reset() # get initial state and prep environment\n",
    "            score = 0\n",
    "            for i in range(200): # run episodes that last 200 frames\n",
    "                show_state(env, i, 'Assault', 'Gen #' + str(gen) + \n",
    "                           ', Team #' + str(teamNum) +\n",
    "                           ', Score: ' + str(score)) # render env\n",
    "\n",
    "                # must transform to at-least int-32 (for my getState to bitshift correctly)\n",
    "                act = agent.act(getState(np.array(state, dtype=np.int32))) # get action from agent\n",
    "\n",
    "                # feedback from env\n",
    "                state, reward, isDone, debug = env.step(act)\n",
    "                score += reward # accumulate reward in score\n",
    "                if isDone:\n",
    "                    break # end early if losing state\n",
    "\n",
    "            agent.reward(score) # must reward agent (if didn't already score)\n",
    "            \n",
    "        curScores.append(score) # store score\n",
    "            \n",
    "    # at end of generation, make summary of scores\n",
    "    summaryScores.append((min(curScores), max(curScores),\n",
    "                    sum(curScores)/len(curScores))) # min, max, avg\n",
    "    trainer.evolve()\n",
    "    \n",
    "#clear_output(wait=True)\n",
    "print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for result in summaryScores:\n",
    "    print(result[0],result[1],result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Generational Selection with Multiprocessing (no graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is just to show a different way to run the API, a far superior way. It uses a different method to get the agents, doesn't use graphics (but can), and uses multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run each agent in this method for parallization.\n",
    "Args:\n",
    "    args: (TpgAgent, envName, scoreList, numEpisodes, numFrames)\n",
    "\"\"\"\n",
    "def runAgent(args):\n",
    "    agent = args[0]\n",
    "    envName = args[1]\n",
    "    scoreList = args[2]\n",
    "    numEpisodes = args[3] # number of times to repeat game\n",
    "    numFrames = args[4] \n",
    "    \n",
    "    # skip if task already done by agent\n",
    "    if agent.taskDone(envName+'-'+str(numFrames)):\n",
    "        print('Agent #' + str(agent.getAgentNum()) + ' can skip.')\n",
    "        scoreList.append((agent.getUid(), agent.getOutcomes()))\n",
    "        return\n",
    "    \n",
    "    env = gym.make(envName)\n",
    "    valActs = range(env.action_space.n) # valid actions, some envs are less\n",
    "    \n",
    "    scoreTotal = 0 # score accumulates over all episodes\n",
    "    for ep in range(numEpisodes): # episode loop\n",
    "        state = env.reset()\n",
    "        scoreEp = 0\n",
    "        numRandFrames = 0\n",
    "        if numEpisodes > 1:\n",
    "            numRandFrames = random.randint(0,30)\n",
    "        for i in range(numFrames): # frame loop\n",
    "            if i < numRandFrames:\n",
    "                _, _, isDone, _ = env.step(env.action_space.sample())\n",
    "                if isDone: # don't count it if lose on random steps\n",
    "                    ep -= 1\n",
    "                continue\n",
    "\n",
    "            act = agent.act(getState(state), valActs=valActs)\n",
    "\n",
    "            # feedback from env\n",
    "            state, reward, isDone, debug = env.step(act)\n",
    "            scoreEp += reward # accumulate reward in score\n",
    "            if isDone:\n",
    "                break # end early if losing state\n",
    "                \n",
    "        print('Agent #' + str(agent.getAgentNum()) + \n",
    "              ' | Ep #' + str(ep) + ' | Score: ' + str(scoreEp))\n",
    "        scoreTotal += scoreEp\n",
    "       \n",
    "    scoreTotal /= numEpisodes\n",
    "    env.close()\n",
    "    agent.reward(scoreTotal, envName+'-'+str(numFrames))\n",
    "    scoreList.append((agent.getUid(), agent.getOutcomes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent #359 | Ep #0 | Score: -21.0\n",
      "Agent #346 | Ep #0 | Score: -100.0\n",
      "Agent #281 | Ep #0 | Score: -36.0\n",
      "Agent #333 | Ep #0 | Score: -41.0\n",
      "Agent #307 | Ep #0 | Score: -41.0\n",
      "Agent #320 | Ep #0 | Score: -54.0\n",
      "Agent #294 | Ep #0 | Score: -56.0\n",
      "Agent #358 | Ep #0 | Score: -54.0\n",
      "Agent #345 | Ep #0 | Score: -30.0\n",
      "Agent #332 | Ep #0 | Score: -41.0\n",
      "Agent #280 | Ep #0 | Score: -30.0\n",
      "Agent #293 | Ep #0 | Score: -100.0\n",
      "Agent #306 | Ep #0 | Score: -54.0\n",
      "Agent #344 | Ep #0 | Score: -100.0\n",
      "Agent #319 | Ep #0 | Score: -36.0\n",
      "Agent #357 | Ep #0 | Score: -41.0\n",
      "Agent #331 | Ep #0 | Score: -30.0\n",
      "Agent #292 | Ep #0 | Score: -54.0\n",
      "Agent #279 | Ep #0 | Score: -41.0\n",
      "Agent #305 | Ep #0 | Score: -79.0\n",
      "Agent #343 | Ep #0 | Score: -36.0\n",
      "Agent #318 | Ep #0 | Score: -36.0\n",
      "Agent #356 | Ep #0 | Score: -54.0\n",
      "Agent #304 | Ep #0 | Score: -100.0\n",
      "Agent #291 | Ep #0 | Score: -98.0\n",
      "Agent #330 | Ep #0 | Score: -41.0\n",
      "Agent #278 | Ep #0 | Score: -48.0\n",
      "Agent #342 | Ep #0 | Score: -54.0\n",
      "Agent #317 | Ep #0 | Score: -41.0\n",
      "Agent #329 | Ep #0 | Score: -100.0\n",
      "Agent #355 | Ep #0 | Score: -54.0\n",
      "Agent #303 | Ep #0 | Score: -41.0\n",
      "Agent #290 | Ep #0 | Score: -41.0\n",
      "Agent #277 | Ep #0 | Score: -54.0\n",
      "Agent #341 | Ep #0 | Score: -30.0\n",
      "Agent #316 | Ep #0 | Score: -54.0\n",
      "Agent #328 | Ep #0 | Score: -41.0\n",
      "Agent #354 | Ep #0 | Score: -54.0\n",
      "Agent #302 | Ep #0 | Score: -30.0\n",
      "Agent #289 | Ep #0 | Score: -54.0\n",
      "Agent #340 | Ep #0 | Score: -38.0\n",
      "Agent #327 | Ep #0 | Score: -100.0\n",
      "Agent #276 | Ep #0 | Score: -54.0\n",
      "Agent #315 | Ep #0 | Score: -39.0\n",
      "Agent #301 | Ep #0 | Score: -100.0\n",
      "Agent #353 | Ep #0 | Score: -54.0\n",
      "Agent #326 | Ep #0 | Score: -100.0\n",
      "Agent #288 | Ep #0 | Score: -36.0\n",
      "Agent #275 | Ep #0 | Score: -100.0\n",
      "Agent #339 | Ep #0 | Score: -100.0\n",
      "Agent #352 | Ep #0 | Score: -100.0\n",
      "Agent #314 | Ep #0 | Score: -36.0\n",
      "Agent #300 | Ep #0 | Score: -30.0\n",
      "Agent #325 | Ep #0 | Score: -41.0\n",
      "Agent #287 | Ep #0 | Score: -54.0\n",
      "Agent #274 | Ep #0 | Score: -30.0\n",
      "Agent #338 | Ep #0 | Score: -41.0\n",
      "Agent #351 | Ep #0 | Score: -41.0\n",
      "Agent #313 | Ep #0 | Score: -100.0\n",
      "Agent #299 | Ep #0 | Score: -33.0\n",
      "Agent #273 | Ep #0 | Score: -100.0\n",
      "Agent #337 | Ep #0 | Score: -100.0\n",
      "Agent #324 | Ep #0 | Score: -59.0\n",
      "Agent #286 | Ep #0 | Score: -54.0\n",
      "Agent #350 | Ep #0 | Score: -54.0\n",
      "Agent #312 | Ep #0 | Score: -54.0\n",
      "Agent #285 | Ep #0 | Score: -100.0\n",
      "Agent #272 | Ep #0 | Score: -54.0\n",
      "Agent #298 | Ep #0 | Score: -41.0\n",
      "Agent #323 | Ep #0 | Score: -41.0\n",
      "Agent #336 | Ep #0 | Score: -41.0\n",
      "Agent #349 | Ep #0 | Score: -40.0\n",
      "Agent #311 | Ep #0 | Score: -41.0\n",
      "Agent #284 | Ep #0 | Score: -41.0\n",
      "Agent #297 | Ep #0 | Score: -54.0\n",
      "Agent #322 | Ep #0 | Score: -50.0\n",
      "Agent #271 | Ep #0 | Score: -54.0\n",
      "Agent #335 | Ep #0 | Score: -33.0\n",
      "Agent #321 | Ep #0 | Score: -100.0\n",
      "Agent #348 | Ep #0 | Score: -40.0\n",
      "Agent #310 | Ep #0 | Score: -41.0\n",
      "Agent #283 | Ep #0 | Score: -36.0\n",
      "Agent #296 | Ep #0 | Score: -32.0\n",
      "Agent #334 | Ep #0 | Score: -41.0\n",
      "Agent #270 | Ep #0 | Score: -41.0\n",
      "Agent #309 | Ep #0 | Score: -100.0\n",
      "Agent #282 | Ep #0 | Score: -95.0\n",
      "Agent #347 | Ep #0 | Score: -30.0\n",
      "Agent #308 | Ep #0 | Score: -100.0\n",
      "Agent #295 | Ep #0 | Score: -41.0\n",
      "Agent #268 | Ep #0 | Score: -41.0\n",
      "Agent #269 | Ep #0 | Score: -41.0\n",
      "Agent #255 | Ep #0 | Score: -100.0\n",
      "Agent #267 | Ep #0 | Score: -100.0\n",
      "Agent #254 | Ep #0 | Score: -100.0\n",
      "Agent #242 | Ep #0 | Score: -54.0\n",
      "Agent #266 | Ep #0 | Score: -41.0\n",
      "Agent #229 | Ep #0 | Score: -73.0\n",
      "Agent #203 | Ep #0 | Score: -100.0\n",
      "Agent #253 | Ep #0 | Score: -41.0\n",
      "Agent #216 | Ep #0 | Score: -30.0\n",
      "Agent #190 | Ep #0 | Score: -11.0\n",
      "Agent #265 | Ep #0 | Score: -36.0\n",
      "Agent #241 | Ep #0 | Score: -38.0\n",
      "Agent #189 | Ep #0 | Score: -100.0\n",
      "Agent #228 | Ep #0 | Score: -54.0\n",
      "Agent #202 | Ep #0 | Score: -41.0\n",
      "Agent #252 | Ep #0 | Score: -50.0\n",
      "Agent #264 | Ep #0 | Score: -100.0\n",
      "Agent #215 | Ep #0 | Score: -36.0\n",
      "Agent #240 | Ep #0 | Score: -100.0\n",
      "Agent #188 | Ep #0 | Score: -41.0\n",
      "Agent #263 | Ep #0 | Score: -100.0\n",
      "Agent #201 | Ep #0 | Score: -100.0\n",
      "Agent #227 | Ep #0 | Score: -19.0\n",
      "Agent #251 | Ep #0 | Score: -100.0\n",
      "Agent #214 | Ep #0 | Score: -54.0\n",
      "Agent #239 | Ep #0 | Score: -100.0\n",
      "Agent #226 | Ep #0 | Score: -100.0\n",
      "Agent #262 | Ep #0 | Score: -100.0\n",
      "Agent #187 | Ep #0 | Score: -61.0\n",
      "Agent #250 | Ep #0 | Score: -41.0\n",
      "Agent #200 | Ep #0 | Score: -41.0\n",
      "Agent #213 | Ep #0 | Score: -18.0\n",
      "Agent #238 | Ep #0 | Score: -41.0\n",
      "Agent #225 | Ep #0 | Score: -100.0\n",
      "Agent #261 | Ep #0 | Score: -38.0\n",
      "Agent #199 | Ep #0 | Score: -100.0\n",
      "Agent #186 | Ep #0 | Score: -36.0\n",
      "Agent #249 | Ep #0 | Score: -54.0\n",
      "Agent #237 | Ep #0 | Score: -100.0\n",
      "Agent #212 | Ep #0 | Score: -54.0\n",
      "Agent #224 | Ep #0 | Score: -54.0\n",
      "Agent #198 | Ep #0 | Score: -99.0\n",
      "Agent #260 | Ep #0 | Score: -54.0\n",
      "Agent #185 | Ep #0 | Score: -19.0\n",
      "Agent #248 | Ep #0 | Score: -41.0\n",
      "Agent #236 | Ep #0 | Score: -100.0\n",
      "Agent #211 | Ep #0 | Score: -54.0\n",
      "Agent #223 | Ep #0 | Score: -50.0\n",
      "Agent #197 | Ep #0 | Score: -50.0\n",
      "Agent #259 | Ep #0 | Score: -50.0\n",
      "Agent #235 | Ep #0 | Score: -34.0\n",
      "Agent #184 | Ep #0 | Score: -54.0\n",
      "Agent #247 | Ep #0 | Score: -45.0\n",
      "Agent #210 | Ep #0 | Score: -89.0\n",
      "Agent #196 | Ep #0 | Score: -100.0\n",
      "Agent #222 | Ep #0 | Score: -54.0\n",
      "Agent #258 | Ep #0 | Score: -33.0\n",
      "Agent #234 | Ep #0 | Score: -54.0\n",
      "Agent #183 | Ep #0 | Score: -54.0\n",
      "Agent #221 | Ep #0 | Score: -100.0\n",
      "Agent #246 | Ep #0 | Score: -41.0\n",
      "Agent #195 | Ep #0 | Score: -30.0\n",
      "Agent #209 | Ep #0 | Score: -41.0\n",
      "Agent #257 | Ep #0 | Score: -100.0\n",
      "Agent #245 | Ep #0 | Score: -100.0\n",
      "Agent #182 | Ep #0 | Score: -54.0\n",
      "Agent #208 | Ep #0 | Score: -100.0\n",
      "Agent #233 | Ep #0 | Score: -36.0\n",
      "Agent #220 | Ep #0 | Score: -30.0\n",
      "Agent #194 | Ep #0 | Score: -75.0\n",
      "Agent #256 | Ep #0 | Score: -41.0\n",
      "Agent #244 | Ep #0 | Score: -41.0\n",
      "Agent #181 | Ep #0 | Score: -54.0\n",
      "Agent #207 | Ep #0 | Score: -54.0\n",
      "Agent #219 | Ep #0 | Score: -45.0\n",
      "Agent #232 | Ep #0 | Score: -54.0\n",
      "Agent #193 | Ep #0 | Score: -41.0\n",
      "Agent #243 | Ep #0 | Score: -43.0\n",
      "Agent #231 | Ep #0 | Score: -100.0\n",
      "Agent #180 | Ep #0 | Score: -54.0\n",
      "Agent #206 | Ep #0 | Score: -54.0\n",
      "Agent #192 | Ep #0 | Score: -100.0\n",
      "Agent #218 | Ep #0 | Score: -36.0\n",
      "Agent #177 | Ep #0 | Score: -39.0\n",
      "Agent #179 | Ep #0 | Score: -100.0\n",
      "Agent #230 | Ep #0 | Score: -41.0\n",
      "Agent #205 | Ep #0 | Score: -100.0\n",
      "Agent #191 | Ep #0 | Score: -41.0\n",
      "Agent #217 | Ep #0 | Score: -100.0\n",
      "Agent #178 | Ep #0 | Score: -100.0\n",
      "Agent #176 | Ep #0 | Score: -38.0\n",
      "Agent #164 | Ep #0 | Score: -30.0\n",
      "Agent #204 | Ep #0 | Score: -54.0\n",
      "Agent #175 | Ep #0 | Score: -36.0\n",
      "Agent #163 | Ep #0 | Score: -54.0\n",
      "Agent #151 | Ep #0 | Score: -38.0\n",
      "Agent #162 | Ep #0 | Score: -100.0\n",
      "Agent #174 | Ep #0 | Score: -54.0\n",
      "Agent #138 | Ep #0 | Score: -54.0\n",
      "Agent #99 | Ep #0 | Score: -100.0\n",
      "Agent #125 | Ep #0 | Score: -96.0\n",
      "Agent #112 | Ep #0 | Score: -54.0\n",
      "Agent #161 | Ep #0 | Score: -100.0\n",
      "Agent #150 | Ep #0 | Score: -38.0\n",
      "Agent #137 | Ep #0 | Score: -100.0\n",
      "Agent #173 | Ep #0 | Score: -18.0\n",
      "Agent #98 | Ep #0 | Score: -36.0\n",
      "Agent #124 | Ep #0 | Score: -30.0\n",
      "Agent #111 | Ep #0 | Score: -36.0\n",
      "Agent #160 | Ep #0 | Score: -36.0\n",
      "Agent #149 | Ep #0 | Score: -30.0\n",
      "Agent #97 | Ep #0 | Score: -100.0\n",
      "Agent #136 | Ep #0 | Score: -36.0\n",
      "Agent #123 | Ep #0 | Score: -100.0\n",
      "Agent #172 | Ep #0 | Score: -30.0\n",
      "Agent #110 | Ep #0 | Score: -41.0\n",
      "Agent #159 | Ep #0 | Score: -48.0\n",
      "Agent #148 | Ep #0 | Score: -50.0\n",
      "Agent #135 | Ep #0 | Score: -100.0\n",
      "Agent #96 | Ep #0 | Score: -26.0\n",
      "Agent #122 | Ep #0 | Score: -41.0\n",
      "Agent #171 | Ep #0 | Score: -54.0\n",
      "Agent #109 | Ep #0 | Score: -54.0\n",
      "Agent #158 | Ep #0 | Score: -54.0\n",
      "Agent #134 | Ep #0 | Score: -19.0\n",
      "Agent #147 | Ep #0 | Score: -100.0\n",
      "Agent #95 | Ep #0 | Score: -30.0\n",
      "Agent #121 | Ep #0 | Score: -100.0\n",
      "Agent #170 | Ep #0 | Score: -54.0\n",
      "Agent #108 | Ep #0 | Score: -54.0\n",
      "Agent #146 | Ep #0 | Score: -100.0\n",
      "Agent #157 | Ep #0 | Score: -100.0\n",
      "Agent #133 | Ep #0 | Score: -38.0\n",
      "Agent #94 | Ep #0 | Score: -54.0\n",
      "Agent #120 | Ep #0 | Score: -54.0\n",
      "Agent #107 | Ep #0 | Score: -35.0\n",
      "Agent #169 | Ep #0 | Score: -30.0\n",
      "Agent #156 | Ep #0 | Score: -54.0\n",
      "Agent #145 | Ep #0 | Score: -54.0\n",
      "Agent #132 | Ep #0 | Score: -100.0\n",
      "Agent #93 | Ep #0 | Score: -30.0\n",
      "Agent #144 | Ep #0 | Score: -100.0\n",
      "Agent #168 | Ep #0 | Score: -100.0\n",
      "Agent #119 | Ep #0 | Score: -54.0\n",
      "Agent #106 | Ep #0 | Score: -41.0\n",
      "Agent #155 | Ep #0 | Score: -30.0\n",
      "Agent #131 | Ep #0 | Score: -54.0\n",
      "Agent #92 | Ep #0 | Score: -30.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent #167 | Ep #0 | Score: -100.0\n",
      "Agent #143 | Ep #0 | Score: -41.0\n",
      "Agent #118 | Ep #0 | Score: -41.0\n",
      "Agent #105 | Ep #0 | Score: -26.0\n",
      "Agent #154 | Ep #0 | Score: 1.0\n",
      "Agent #130 | Ep #0 | Score: -30.0\n",
      "Agent #91 | Ep #0 | Score: -100.0\n",
      "Agent #142 | Ep #0 | Score: -13.0\n",
      "Agent #166 | Ep #0 | Score: -71.0\n",
      "Agent #117 | Ep #0 | Score: -52.0\n",
      "Agent #129 | Ep #0 | Score: -100.0\n",
      "Agent #104 | Ep #0 | Score: -41.0\n",
      "Agent #153 | Ep #0 | Score: -30.0\n",
      "Agent #90 | Ep #0 | Score: -100.0\n",
      "Agent #141 | Ep #0 | Score: -100.0\n",
      "Agent #165 | Ep #0 | Score: -100.0\n",
      "Agent #152 | Ep #0 | Score: -100.0\n",
      "Agent #103 | Ep #0 | Score: -41.0\n",
      "Agent #128 | Ep #0 | Score: -54.0\n",
      "Agent #116 | Ep #0 | Score: -54.0\n",
      "Agent #89 | Ep #0 | Score: -39.0\n",
      "Agent #140 | Ep #0 | Score: -41.0\n",
      "Agent #127 | Ep #0 | Score: -100.0\n",
      "Agent #102 | Ep #0 | Score: -100.0\n",
      "Agent #86 | Ep #0 | Score: -100.0\n",
      "Agent #115 | Ep #0 | Score: -54.0\n",
      "Agent #73 | Ep #0 | Score: -100.0\n",
      "Agent #126 | Ep #0 | Score: -100.0\n",
      "Agent #88 | Ep #0 | Score: -36.0\n",
      "Agent #139 | Ep #0 | Score: -54.0\n",
      "Agent #101 | Ep #0 | Score: -30.0\n",
      "Agent #85 | Ep #0 | Score: -30.0\n",
      "Agent #72 | Ep #0 | Score: -100.0\n",
      "Agent #114 | Ep #0 | Score: -30.0\n",
      "Agent #87 | Ep #0 | Score: -41.0\n",
      "Agent #100 | Ep #0 | Score: -39.0\n",
      "Agent #84 | Ep #0 | Score: -41.0\n",
      "Agent #113 | Ep #0 | Score: -100.0\n",
      "Agent #47 | Ep #0 | Score: -56.0\n",
      "Agent #71 | Ep #0 | Score: -54.0\n",
      "Agent #60 | Ep #0 | Score: -40.0\n",
      "Agent #34 | Ep #0 | Score: -100.0\n",
      "Agent #59 | Ep #0 | Score: -100.0\n",
      "Agent #70 | Ep #0 | Score: -54.0\n",
      "Agent #83 | Ep #0 | Score: -41.0\n",
      "Agent #46 | Ep #0 | Score: -41.0\n",
      "Agent #33 | Ep #0 | Score: -100.0\n",
      "Agent #8 | Ep #0 | Score: -30.0\n",
      "Agent #58 | Ep #0 | Score: -36.0\n",
      "Agent #21 | Ep #0 | Score: -30.0\n",
      "Agent #45 | Ep #0 | Score: -100.0\n",
      "Agent #69 | Ep #0 | Score: -100.0\n",
      "Agent #82 | Ep #0 | Score: -54.0\n",
      "Agent #7 | Ep #0 | Score: -100.0\n",
      "Agent #32 | Ep #0 | Score: -41.0\n",
      "Agent #57 | Ep #0 | Score: -54.0\n",
      "Agent #44 | Ep #0 | Score: -30.0\n",
      "Agent #20 | Ep #0 | Score: -41.0\n",
      "Agent #68 | Ep #0 | Score: -30.0\n",
      "Agent #81 | Ep #0 | Score: -54.0\n",
      "Agent #31 | Ep #0 | Score: -100.0\n",
      "Agent #6 | Ep #0 | Score: -30.0\n",
      "Agent #80 | Ep #0 | Score: -100.0\n",
      "Agent #43 | Ep #0 | Score: -100.0\n",
      "Agent #30 | Ep #0 | Score: -100.0\n",
      "Agent #56 | Ep #0 | Score: -30.0\n",
      "Agent #5 | Ep #0 | Score: -100.0\n",
      "Agent #19 | Ep #0 | Score: -36.0\n",
      "Agent #67 | Ep #0 | Score: -54.0\n",
      "Agent #79 | Ep #0 | Score: -26.0\n",
      "Agent #55 | Ep #0 | Score: -100.0\n",
      "Agent #42 | Ep #0 | Score: -100.0\n",
      "Agent #18 | Ep #0 | Score: -100.0\n",
      "Agent #29 | Ep #0 | Score: -41.0\n",
      "Agent #66 | Ep #0 | Score: -54.0\n",
      "Agent #4 | Ep #0 | Score: -30.0\n",
      "Agent #54 | Ep #0 | Score: -100.0\n",
      "Agent #78 | Ep #0 | Score: -32.0\n",
      "Agent #17 | Ep #0 | Score: -30.0\n",
      "Agent #41 | Ep #0 | Score: -50.0\n",
      "Agent #28 | Ep #0 | Score: -100.0\n",
      "Agent #3 | Ep #0 | Score: -41.0\n",
      "Agent #65 | Ep #0 | Score: -54.0\n",
      "Agent #53 | Ep #0 | Score: -100.0\n",
      "Agent #16 | Ep #0 | Score: -26.0\n",
      "Agent #27 | Ep #0 | Score: -100.0\n",
      "Agent #40 | Ep #0 | Score: -30.0\n",
      "Agent #77 | Ep #0 | Score: -22.0\n",
      "Agent #64 | Ep #0 | Score: -30.0\n",
      "Agent #2 | Ep #0 | Score: -50.0\n",
      "Agent #52 | Ep #0 | Score: -36.0\n",
      "Agent #15 | Ep #0 | Score: -54.0\n",
      "Agent #39 | Ep #0 | Score: -41.0\n",
      "Agent #26 | Ep #0 | Score: -41.0\n",
      "Agent #76 | Ep #0 | Score: -54.0\n",
      "Agent #51 | Ep #0 | Score: -100.0\n",
      "Agent #63 | Ep #0 | Score: -36.0\n",
      "Agent #1 | Ep #0 | Score: -54.0\n",
      "Agent #14 | Ep #0 | Score: -28.0\n",
      "Agent #38 | Ep #0 | Score: -41.0\n",
      "Agent #75 | Ep #0 | Score: -41.0\n",
      "Agent #25 | Ep #0 | Score: -54.0\n",
      "Agent #62 | Ep #0 | Score: -36.0\n",
      "Agent #50 | Ep #0 | Score: -41.0\n",
      "Agent #0 | Ep #0 | Score: -60.0\n",
      "Agent #37 | Ep #0 | Score: -100.0\n",
      "Agent #13 | Ep #0 | Score: -54.0\n",
      "Agent #74 | Ep #0 | Score: -54.0\n",
      "Agent #49 | Ep #0 | Score: -100.0\n",
      "Agent #61 | Ep #0 | Score: -41.0\n",
      "Agent #24 | Ep #0 | Score: -54.0\n",
      "Agent #36 | Ep #0 | Score: -54.0\n",
      "Agent #23 | Ep #0 | Score: -100.0\n",
      "Agent #12 | Ep #0 | Score: -41.0\n",
      "Agent #48 | Ep #0 | Score: -17.0\n",
      "Agent #22 | Ep #0 | Score: -36.0\n",
      "Agent #35 | Ep #0 | Score: -16.0\n",
      "Agent #11 | Ep #0 | Score: -41.0\n",
      "Agent #10 | Ep #0 | Score: -54.0\n",
      "Agent #9 | Ep #0 | Score: -23.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TpgTrainer' object has no attribute 'scoreStats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-35fab0f4f6ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# an easier way to track stats than the above example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mscoreStats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoreStats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mallScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoreStats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoreStats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoreStats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'average'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TpgTrainer' object has no attribute 'scoreStats'"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "trainer = TpgTrainer(actions=range(7), teamPopSize=360)\n",
    "\n",
    "processes = 7 \n",
    "man = mp.Manager()\n",
    "pool = mp.Pool(processes=processes, maxtasksperchild=1)\n",
    "    \n",
    "allScores = [] # track all scores each generation\n",
    "\n",
    "for gen in range(30): # do 30 generations of training\n",
    "    scoreList = man.list()\n",
    "    \n",
    "    # get agents, noRef to not hold reference to trainer in each one\n",
    "    # don't need reference to trainer in multiprocessing\n",
    "    agents = trainer.getAllAgents(noRef=True) # swap out agents only at start of generation\n",
    "\n",
    "    # run the agents\n",
    "    pool.map(runAgent, \n",
    "        [(agent, 'Boxing-v0', scoreList, 1, 18000)\n",
    "        for agent in agents])\n",
    "    \n",
    "    # apply scores, must do this when multiprocessing\n",
    "    # because agents can't refer to trainer\n",
    "    teams = trainer.applyScores(scoreList)\n",
    "    # important to remember to set tasks right, unless not using task names\n",
    "    # task name set in runAgent()\n",
    "    trainer.evolve(tasks=['Boxing-v0-18000']) # go into next gen\n",
    "    \n",
    "    # an easier way to track stats than the above example\n",
    "    scoreStats = trainer.scoreStats\n",
    "    allScores.append((scoreStats['min'], scoreStats['max'], scoreStats['average']))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "    print('Results so far: ' + str(allScores))\n",
    "    \n",
    "clear_output(wait=True)\n",
    "print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for score in allScores:\n",
    "    print(score[0],score[1],score[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the results above, boxing is a particularly difficult game that takes a while for improvement. The max score will sometimes take a dip because the root team that had the max score may become a child of another root team during a mutation, making it no longer a root team. But this new mutation either shortly disapears if it is no good and the old root team is restored, or, it will perform well and stay for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tournament Selection (In progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (Seconds): 45202.94136309624\n",
      "Results: [(0, 16905.0, 50.91867469879518), (0, 31521.0, 99.43533123028391), (0, 47733.0, 157.01644736842104), (0, 59262.0, 196.2317880794702), (0, 63903.0, 215.16161616161617), (0, 66339.0, 230.34375), (0, 68754.0, 242.09154929577466), (0, 68460.0, 244.5), (0, 73290.0, 251.8556701030928), (0, 70371.0, 256.8284671532847)]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "lock = mp.Lock()\n",
    "\n",
    "trainer = TpgTrainer(actions=range(7), teamPopSizeInit=360)\n",
    "\n",
    "processes = 4 # how many to run concurrently (4 is best for my local desktop)\n",
    "\n",
    "m = mp.Manager()\n",
    "envQueue = m.Queue()\n",
    "# each process needs its own environment\n",
    "for i in range(processes):\n",
    "    envQueue.put(gym.make('Assault-v0'))\n",
    "\n",
    "pool = mp.Pool(processes=processes)\n",
    "    \n",
    "summaryScores = [] # record score summaries for population\n",
    "    \n",
    "# tournament loop \n",
    "# 450 tournaments of size 8 approximately equals 10 generations\n",
    "# *at pop size 360\n",
    "for tourn in range(450): \n",
    "    scoreQueue = m.Queue() # hold agents when finish, to actually apply score\n",
    "\n",
    "    # run tournament\n",
    "    # skipTasks=[] so we get all agents, even if already scored,\n",
    "    # just to report the obtained score for all agents.\n",
    "    pool.map(runAgent, \n",
    "                 [(agent, envQueue, scoreQueue) \n",
    "                  for agent in trainer.getTournamentAgents()])\n",
    "    \n",
    "    scores = [] # convert scores into list\n",
    "    while not scoreQueue.empty():\n",
    "        scores.append(scoreQueue.get())\n",
    "    \n",
    "    # apply scores\n",
    "    teams = trainer.applyScores(scores) # get teams from trainer\n",
    "    trainer.evolve(tourneyTeams=teams,tasks=[]) # evolve tournament players\n",
    "    \n",
    "    # report score once equivalent to a generation\n",
    "    if (tourn+1) % 45 == 0:\n",
    "        scoreStats = trainer.generateScoreStats()\n",
    "\n",
    "        # at end of generation, make summary of scores\n",
    "        summaryScores.append((scoreStats['min'], \n",
    "                        scoreStats['max'],\n",
    "                        scoreStats['average'])) # min, max, avg\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "        print('Results so far: ' + str(summaryScores))\n",
    "    \n",
    "clear_output(wait=True)\n",
    "print('Time Taken (Seconds): ' + str(time.time() - tStart))\n",
    "print('Results: ' + str(summaryScores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oaigym]",
   "language": "python",
   "name": "conda-env-oaigym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

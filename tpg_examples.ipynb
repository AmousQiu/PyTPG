{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPG Examples\n",
    "This document shows how to use the PyTPG API. We make use of OpenAI Gym to run examples, and we assume you already have PyTPG installed, see the readme for installation instructions for PyTPG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# how to render in Jupyter: \n",
    "# https://stackoverflow.com/questions/40195740/how-to-run-openai-gym-render-over-a-server\n",
    "# https://www.youtube.com/watch?v=O84KgRt6AJI\n",
    "def show_state(env, step=0, name='', info=''):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (name, step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "# To transform pixel matrix to a single vector.\n",
    "def getState(inState):\n",
    "    # each row is all 1 color\n",
    "    rgbRows = np.reshape(inState,(len(inState[0])*len(inState), 3)).T\n",
    "\n",
    "    # add each with appropriate shifting\n",
    "    # get RRRRRRRR GGGGGGGG BBBBBBBB\n",
    "    return np.add(np.left_shift(rgbRows[0], 16),\n",
    "        np.add(np.left_shift(rgbRows[1], 8), rgbRows[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Assault-v0') # make the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(7)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space) # learn size of action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import to do training\n",
    "from tpg.trainer import Trainer\n",
    "# import to run an agent (always needed)\n",
    "from tpg.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Basic Generational Selection (with graphics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (Hours): 8926443.869876862\n",
      "Results:\n",
      "Min, Max, Avg\n",
      "0.0 336.0 54.6\n",
      "0.0 252.0 35.7\n",
      "0.0 399.0 135.54545454545453\n",
      "0.0 357.0 210.0\n",
      "0.0 378.0 175.63636363636363\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time # for tracking time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "# first create an instance of the TpgTrainer\n",
    "# this creates the whole population and everything\n",
    "# teamPopSize should realistically be at-least 100\n",
    "trainer = Trainer(actions=range(7), teamPopSize=20, rTeamPopSize=20) \n",
    "\n",
    "curScores = [] # hold scores in a generation\n",
    "summaryScores = [] # record score summaries for each gen (min, max, avg)\n",
    "\n",
    "# 5 generations isn't much (not even close), but some improvements\n",
    "# should be seen.\n",
    "for gen in range(5): # generation loop\n",
    "    curScores = [] # new list per gen\n",
    "    \n",
    "    agents = trainer.getAgents()\n",
    "    \n",
    "    while True: # loop to go through agents\n",
    "        teamNum = len(agents)\n",
    "        agent = agents.pop()\n",
    "        if agent is None:\n",
    "            break # no more agents, so proceed to next gen\n",
    "        \n",
    "        state = env.reset() # get initial state and prep environment\n",
    "        score = 0\n",
    "        for i in range(500): # run episodes that last 500 frames\n",
    "            show_state(env, i, 'Assault', 'Gen #' + str(gen) + \n",
    "                       ', Team #' + str(teamNum) +\n",
    "                       ', Score: ' + str(score)) # render env\n",
    "            \n",
    "            # get action from agent\n",
    "            # must transform to at-least int-32 (for my getState to bitshift correctly)\n",
    "            act = agent.act(getState(np.array(state, dtype=np.int32))) \n",
    "\n",
    "            # feedback from env\n",
    "            state, reward, isDone, debug = env.step(act)\n",
    "            score += reward # accumulate reward in score\n",
    "            if isDone:\n",
    "                break # end early if losing state\n",
    "\n",
    "        agent.reward(score) # must reward agent (if didn't already score)\n",
    "            \n",
    "        curScores.append(score) # store score\n",
    "        \n",
    "        if len(agents) == 0:\n",
    "            break\n",
    "            \n",
    "    # at end of generation, make summary of scores\n",
    "    summaryScores.append((min(curScores), max(curScores),\n",
    "                    sum(curScores)/len(curScores))) # min, max, avg\n",
    "    trainer.evolve()\n",
    "    \n",
    "#clear_output(wait=True)\n",
    "print('Time Taken (Hours): ' + str((time.time() - tStart)*3600))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for result in summaryScores:\n",
    "    print(result[0],result[1],result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Generational Selection with Multiprocessing (no graphics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is just to show a different way to run the API, a far superior way. It uses a different method to get the agents, doesn't use graphics (but can), and uses multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run each agent in this method for parallization.\n",
    "Args:\n",
    "    args: (TpgAgent, envName, scoreList, numEpisodes, numFrames)\n",
    "\"\"\"\n",
    "def runAgent(args):\n",
    "    agent = args[0]\n",
    "    envName = args[1]\n",
    "    scoreList = args[2]\n",
    "    numEpisodes = args[3] # number of times to repeat game\n",
    "    numFrames = args[4] \n",
    "    \n",
    "    # skip if task already done by agent\n",
    "    if agent.taskDone(envName+'-'+str(numFrames)):\n",
    "        print('Agent #' + str(agent.agentNum) + ' can skip.')\n",
    "        scoreList.append((agent.team.id, agent.team.outcomes))\n",
    "        return\n",
    "    \n",
    "    env = gym.make(envName)\n",
    "    valActs = range(env.action_space.n) # valid actions, some envs are less\n",
    "    \n",
    "    scoreTotal = 0 # score accumulates over all episodes\n",
    "    for ep in range(numEpisodes): # episode loop\n",
    "        state = env.reset()\n",
    "        scoreEp = 0\n",
    "        numRandFrames = 0\n",
    "        if numEpisodes > 1:\n",
    "            numRandFrames = random.randint(0,30)\n",
    "        for i in range(numFrames): # frame loop\n",
    "            if i < numRandFrames:\n",
    "                _, _, isDone, _ = env.step(env.action_space.sample())\n",
    "                if isDone: # don't count it if lose on random steps\n",
    "                    ep -= 1\n",
    "                continue\n",
    "\n",
    "            act = agent.act(getState(state))\n",
    "\n",
    "            # feedback from env\n",
    "            state, reward, isDone, debug = env.step(act)\n",
    "            scoreEp += reward # accumulate reward in score\n",
    "            if isDone:\n",
    "                break # end early if losing state\n",
    "                \n",
    "        print('Agent #' + str(agent.agentNum) + \n",
    "              ' | Ep #' + str(ep) + ' | Score: ' + str(scoreEp))\n",
    "        scoreTotal += scoreEp\n",
    "       \n",
    "    scoreTotal /= numEpisodes\n",
    "    env.close()\n",
    "    agent.reward(scoreTotal, envName)\n",
    "    scoreList.append((agent.team.id, agent.team.outcomes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken (Hours): 5834766.043281555\n",
      "Results so far: [(-100.0, 5.0, -47.205), (-100.0, -1.0, -34.75), (-100.0, 1.0, -33.1576354679803), (-100.0, 1.0, -32.432038834951456), (-100.0, 10.0, -31.102678571428573), (-100.0, 7.0, -34.291079812206576), (-100.0, 11.0, -29.08450704225352), (-100.0, 1.0, -30.162037037037038), (-100.0, 10.0, -35.02283105022831), (-100.0, 1.0, -29.61187214611872), (-100.0, 3.0, -27.193548387096776), (-100.0, 3.0, -28.34722222222222), (-100.0, 5.0, -35.15), (-100.0, 2.0, -34.56398104265403), (-100.0, 3.0, -31.91826923076923), (-100.0, 6.0, -26.727272727272727)]\n",
      "Time Taken (Hours): 5834766.043281555\n",
      "Results so far: [(-100.0, 5.0, -47.205), (-100.0, -1.0, -34.75), (-100.0, 1.0, -33.1576354679803), (-100.0, 1.0, -32.432038834951456), (-100.0, 10.0, -31.102678571428573), (-100.0, 7.0, -34.291079812206576), (-100.0, 11.0, -29.08450704225352), (-100.0, 1.0, -30.162037037037038), (-100.0, 10.0, -35.02283105022831), (-100.0, 1.0, -29.61187214611872), (-100.0, 3.0, -27.193548387096776), (-100.0, 3.0, -28.34722222222222), (-100.0, 5.0, -35.15), (-100.0, 2.0, -34.56398104265403), (-100.0, 3.0, -31.91826923076923), (-100.0, 6.0, -26.727272727272727)]\n",
      "Agent #50 | Ep #0 | Score: -25.0\n",
      "Agent #40 | Ep #0 | Score: -25.0\n",
      "Agent #0 | Ep #0 | Score: -25.0\n",
      "Agent #10 | Ep #0 | Score: -48.0\n",
      "Agent #30 | Ep #0 | Score: -25.0\n",
      "Agent #20 | Ep #0 | Score: -25.0\n",
      "Agent #51 | Ep #0 | Score: -41.0\n",
      "Agent #31 | Ep #0 | Score: -25.0\n",
      "Agent #41 | Ep #0 | Score: -25.0\n",
      "Agent #1 | Ep #0 | Score: -25.0\n",
      "Agent #11 | Ep #0 | Score: -14.0\n",
      "Agent #21 | Ep #0 | Score: -23.0\n",
      "Agent #52 | Ep #0 | Score: -25.0\n",
      "Agent #32 | Ep #0 | Score: -30.0\n",
      "Agent #42 | Ep #0 | Score: 3.0\n",
      "Agent #12 | Ep #0 | Score: -36.0\n",
      "Agent #2 | Ep #0 | Score: -25.0\n",
      "Agent #22 | Ep #0 | Score: -25.0\n",
      "Agent #53 | Ep #0 | Score: -25.0\n",
      "Agent #3 | Ep #0 | Score: -9.0\n",
      "Agent #13 | Ep #0 | Score: 0.0\n",
      "Agent #33 | Ep #0 | Score: -1.0\n",
      "Agent #43 | Ep #0 | Score: -25.0\n",
      "Agent #23 | Ep #0 | Score: -8.0\n",
      "Agent #54 | Ep #0 | Score: -25.0\n",
      "Agent #14 | Ep #0 | Score: -19.0\n",
      "Agent #34 | Ep #0 | Score: -1.0\n",
      "Agent #4 | Ep #0 | Score: -25.0\n",
      "Agent #44 | Ep #0 | Score: -25.0\n",
      "Agent #24 | Ep #0 | Score: -25.0\n",
      "Agent #55 | Ep #0 | Score: -25.0\n",
      "Agent #15 | Ep #0 | Score: -25.0\n",
      "Agent #35 | Ep #0 | Score: -9.0\n",
      "Agent #45 | Ep #0 | Score: -25.0\n",
      "Agent #5 | Ep #0 | Score: -22.0\n",
      "Agent #25 | Ep #0 | Score: -25.0\n",
      "Agent #56 | Ep #0 | Score: -25.0\n",
      "Agent #16 | Ep #0 | Score: -25.0\n",
      "Agent #46 | Ep #0 | Score: -25.0\n",
      "Agent #36 | Ep #0 | Score: -25.0\n",
      "Agent #6 | Ep #0 | Score: -41.0\n",
      "Agent #26 | Ep #0 | Score: -25.0\n",
      "Agent #57 | Ep #0 | Score: -6.0\n",
      "Agent #17 | Ep #0 | Score: -25.0\n",
      "Agent #47 | Ep #0 | Score: -25.0\n",
      "Agent #7 | Ep #0 | Score: -25.0\n",
      "Agent #37 | Ep #0 | Score: -5.0\n",
      "Agent #27 | Ep #0 | Score: -25.0\n",
      "Agent #58 | Ep #0 | Score: -25.0\n",
      "Agent #8 | Ep #0 | Score: -25.0\n",
      "Agent #18 | Ep #0 | Score: -54.0\n",
      "Agent #48 | Ep #0 | Score: -25.0\n",
      "Agent #38 | Ep #0 | Score: -25.0\n",
      "Agent #59 | Ep #0 | Score: 0.0\n",
      "Agent #28 | Ep #0 | Score: -25.0\n",
      "Agent #9 | Ep #0 | Score: -25.0\n",
      "Agent #19 | Ep #0 | Score: -25.0\n",
      "Agent #49 | Ep #0 | Score: -25.0\n",
      "Agent #39 | Ep #0 | Score: -25.0\n",
      "Agent #60 | Ep #0 | Score: -19.0\n",
      "Agent #29 | Ep #0 | Score: -23.0\n",
      "Agent #70 | Ep #0 | Score: -25.0\n",
      "Agent #80 | Ep #0 | Score: -25.0\n",
      "Agent #90 | Ep #0 | Score: 0.0\n",
      "Agent #61 | Ep #0 | Score: -25.0\n",
      "Agent #100 | Ep #0 | Score: -25.0\n",
      "Agent #71 | Ep #0 | Score: -14.0\n",
      "Agent #110 | Ep #0 | Score: -25.0\n",
      "Agent #81 | Ep #0 | Score: -25.0\n",
      "Agent #62 | Ep #0 | Score: -25.0\n",
      "Agent #91 | Ep #0 | Score: 4.0\n",
      "Agent #101 | Ep #0 | Score: -19.0\n",
      "Agent #72 | Ep #0 | Score: -25.0\n",
      "Agent #63 | Ep #0 | Score: -22.0\n",
      "Agent #92 | Ep #0 | Score: -8.0\n",
      "Agent #111 | Ep #0 | Score: 0.0\n",
      "Agent #82 | Ep #0 | Score: -25.0\n",
      "Agent #102 | Ep #0 | Score: -30.0\n",
      "Agent #73 | Ep #0 | Score: -25.0\n",
      "Agent #93 | Ep #0 | Score: -9.0\n",
      "Agent #112 | Ep #0 | Score: -41.0\n",
      "Agent #64 | Ep #0 | Score: -22.0\n",
      "Agent #103 | Ep #0 | Score: -25.0\n",
      "Agent #83 | Ep #0 | Score: -8.0\n",
      "Agent #74 | Ep #0 | Score: -24.0\n",
      "Agent #94 | Ep #0 | Score: -37.0\n",
      "Agent #84 | Ep #0 | Score: -25.0\n",
      "Agent #113 | Ep #0 | Score: -25.0\n",
      "Agent #104 | Ep #0 | Score: -16.0\n",
      "Agent #65 | Ep #0 | Score: -25.0\n",
      "Agent #95 | Ep #0 | Score: -19.0\n",
      "Agent #75 | Ep #0 | Score: -4.0\n",
      "Agent #105 | Ep #0 | Score: -19.0\n",
      "Agent #114 | Ep #0 | Score: -20.0\n",
      "Agent #85 | Ep #0 | Score: -7.0\n",
      "Agent #66 | Ep #0 | Score: -25.0\n",
      "Agent #96 | Ep #0 | Score: -14.0\n",
      "Agent #76 | Ep #0 | Score: -25.0\n",
      "Agent #106 | Ep #0 | Score: -100.0\n",
      "Agent #86 | Ep #0 | Score: -7.0\n",
      "Agent #115 | Ep #0 | Score: -55.0\n",
      "Agent #67 | Ep #0 | Score: -25.0\n",
      "Agent #97 | Ep #0 | Score: -9.0\n",
      "Agent #77 | Ep #0 | Score: -25.0\n",
      "Agent #87 | Ep #0 | Score: -7.0\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "tStart = time.time()\n",
    "\n",
    "envName = 'Boxing-v0'\n",
    "# get num actions\n",
    "env = gym.make(envName)\n",
    "acts = env.action_space.n\n",
    "del env\n",
    "\n",
    "trainer = Trainer(actions=range(acts), teamPopSize=200, rTeamPopSize=200)\n",
    "\n",
    "processes = 6 \n",
    "man = mp.Manager()\n",
    "pool = mp.Pool(processes=processes, maxtasksperchild=1)\n",
    "    \n",
    "allScores = [] # track all scores each generation\n",
    "\n",
    "for gen in range(100): # do 30 generations of training\n",
    "    scoreList = man.list()\n",
    "    \n",
    "    # get agents, noRef to not hold reference to trainer in each one\n",
    "    # don't need reference to trainer in multiprocessing\n",
    "    agents = trainer.getAgents() # swap out agents only at start of generation\n",
    "\n",
    "    # run the agents\n",
    "    pool.map(runAgent, \n",
    "        [(agent, envName, scoreList, 1, 18000)\n",
    "        for agent in agents])\n",
    "    \n",
    "    # apply scores, must do this when multiprocessing\n",
    "    # because agents can't refer to trainer\n",
    "    teams = trainer.applyScores(scoreList)\n",
    "    # important to remember to set tasks right, unless not using task names\n",
    "    # task name set in runAgent()\n",
    "    trainer.evolve(task=envName) # go into next gen\n",
    "    \n",
    "    # an easier way to track stats than the above example\n",
    "    scoreStats = trainer.fitnessStats\n",
    "    allScores.append((scoreStats['min'], scoreStats['max'], scoreStats['average']))\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print('Time Taken (Hours): ' + str((time.time() - tStart)*3600))\n",
    "    print('Results so far: ' + str(allScores))\n",
    "    \n",
    "clear_output(wait=True)\n",
    "print('Time Taken (Hours): ' + str((time.time() - tStart)*3600))\n",
    "print('Results:\\nMin, Max, Avg')\n",
    "for score in allScores:\n",
    "    print(score[0],score[1],score[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the results above, boxing is a particularly difficult game that takes a while for improvement. The max score will sometimes take a dip because the root team that had the max score may become a child of another root team during a mutation, making it no longer a root team. But this new mutation either shortly disapears if it is no good and the old root team is restored, or, it will perform well and stay for a while."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:oaigym] *",
   "language": "python",
   "name": "conda-env-oaigym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
